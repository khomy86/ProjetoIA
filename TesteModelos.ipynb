{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEhUX_Jrxeft",
        "outputId": "818726b0-16d7-4161-e748-76dab178c725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 26 23:23:05 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0              29W /  70W |    213MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n",
            "Dispositivo utilizado: cuda\n",
            "Dados carregados com sucesso!\n",
            "\n",
            "🚀 Iniciando treino do XGBoost...\n",
            "[0]\tval-rmse:1.07123\n",
            "[100]\tval-rmse:0.21473\n",
            "[200]\tval-rmse:0.17785\n",
            "[300]\tval-rmse:0.17116\n",
            "[400]\tval-rmse:0.17066\n",
            "[500]\tval-rmse:0.17032\n",
            "[600]\tval-rmse:0.16999\n",
            "[700]\tval-rmse:0.16988\n",
            "[781]\tval-rmse:0.16980\n",
            "\n",
            "🧠 Iniciando treino da Rede Neural...\n",
            "Epoch 001 | Loss: 3.1941 | Val Loss: 0.9876\n",
            "Epoch 002 | Loss: 0.2796 | Val Loss: 0.0479\n",
            "Epoch 003 | Loss: 0.0530 | Val Loss: 0.0390\n",
            "Epoch 004 | Loss: 0.0462 | Val Loss: 0.0378\n",
            "Epoch 005 | Loss: 0.0430 | Val Loss: 0.0353\n",
            "Epoch 006 | Loss: 0.0411 | Val Loss: 0.0347\n",
            "Epoch 007 | Loss: 0.0399 | Val Loss: 0.0341\n",
            "Epoch 008 | Loss: 0.0387 | Val Loss: 0.0332\n",
            "Epoch 009 | Loss: 0.0379 | Val Loss: 0.0328\n",
            "Epoch 010 | Loss: 0.0371 | Val Loss: 0.0323\n",
            "Epoch 011 | Loss: 0.0363 | Val Loss: 0.0325\n",
            "Epoch 012 | Loss: 0.0357 | Val Loss: 0.0316\n",
            "Epoch 013 | Loss: 0.0354 | Val Loss: 0.0316\n",
            "Epoch 014 | Loss: 0.0347 | Val Loss: 0.0313\n",
            "Epoch 015 | Loss: 0.0346 | Val Loss: 0.0304\n",
            "Epoch 016 | Loss: 0.0338 | Val Loss: 0.0304\n",
            "Epoch 017 | Loss: 0.0335 | Val Loss: 0.0298\n",
            "Epoch 018 | Loss: 0.0332 | Val Loss: 0.0292\n",
            "Epoch 019 | Loss: 0.0326 | Val Loss: 0.0289\n",
            "Epoch 020 | Loss: 0.0326 | Val Loss: 0.0286\n",
            "Epoch 021 | Loss: 0.0318 | Val Loss: 0.0287\n",
            "Epoch 022 | Loss: 0.0318 | Val Loss: 0.0286\n",
            "Epoch 023 | Loss: 0.0311 | Val Loss: 0.0284\n",
            "Epoch 024 | Loss: 0.0309 | Val Loss: 0.0290\n",
            "Epoch 025 | Loss: 0.0305 | Val Loss: 0.0272\n",
            "Epoch 026 | Loss: 0.0302 | Val Loss: 0.0275\n",
            "Epoch 027 | Loss: 0.0302 | Val Loss: 0.0274\n",
            "Epoch 028 | Loss: 0.0300 | Val Loss: 0.0265\n",
            "Epoch 029 | Loss: 0.0294 | Val Loss: 0.0267\n",
            "Epoch 030 | Loss: 0.0292 | Val Loss: 0.0268\n",
            "Epoch 031 | Loss: 0.0291 | Val Loss: 0.0257\n",
            "Epoch 032 | Loss: 0.0286 | Val Loss: 0.0255\n",
            "Epoch 033 | Loss: 0.0283 | Val Loss: 0.0261\n",
            "Epoch 034 | Loss: 0.0283 | Val Loss: 0.0253\n",
            "Epoch 035 | Loss: 0.0277 | Val Loss: 0.0248\n",
            "Epoch 036 | Loss: 0.0279 | Val Loss: 0.0253\n",
            "Epoch 037 | Loss: 0.0276 | Val Loss: 0.0244\n",
            "Epoch 038 | Loss: 0.0274 | Val Loss: 0.0246\n",
            "Epoch 039 | Loss: 0.0274 | Val Loss: 0.0245\n",
            "Epoch 040 | Loss: 0.0268 | Val Loss: 0.0239\n",
            "Epoch 041 | Loss: 0.0269 | Val Loss: 0.0241\n",
            "Epoch 042 | Loss: 0.0271 | Val Loss: 0.0245\n",
            "Epoch 043 | Loss: 0.0268 | Val Loss: 0.0238\n",
            "Epoch 044 | Loss: 0.0264 | Val Loss: 0.0236\n",
            "Epoch 045 | Loss: 0.0264 | Val Loss: 0.0236\n",
            "Epoch 046 | Loss: 0.0263 | Val Loss: 0.0238\n",
            "Epoch 047 | Loss: 0.0262 | Val Loss: 0.0241\n",
            "Epoch 048 | Loss: 0.0261 | Val Loss: 0.0236\n",
            "Epoch 049 | Loss: 0.0259 | Val Loss: 0.0230\n",
            "Epoch 050 | Loss: 0.0257 | Val Loss: 0.0236\n",
            "Epoch 051 | Loss: 0.0256 | Val Loss: 0.0228\n",
            "Epoch 052 | Loss: 0.0255 | Val Loss: 0.0228\n",
            "Epoch 053 | Loss: 0.0255 | Val Loss: 0.0230\n",
            "Epoch 054 | Loss: 0.0256 | Val Loss: 0.0231\n",
            "Epoch 055 | Loss: 0.0254 | Val Loss: 0.0226\n",
            "Epoch 056 | Loss: 0.0249 | Val Loss: 0.0225\n",
            "Epoch 057 | Loss: 0.0250 | Val Loss: 0.0224\n",
            "Epoch 058 | Loss: 0.0249 | Val Loss: 0.0220\n",
            "Epoch 059 | Loss: 0.0248 | Val Loss: 0.0222\n",
            "Epoch 060 | Loss: 0.0246 | Val Loss: 0.0226\n",
            "Epoch 061 | Loss: 0.0247 | Val Loss: 0.0218\n",
            "Epoch 062 | Loss: 0.0246 | Val Loss: 0.0229\n",
            "Epoch 063 | Loss: 0.0246 | Val Loss: 0.0216\n",
            "Epoch 064 | Loss: 0.0243 | Val Loss: 0.0220\n",
            "Epoch 065 | Loss: 0.0241 | Val Loss: 0.0218\n",
            "Epoch 066 | Loss: 0.0241 | Val Loss: 0.0214\n",
            "Epoch 067 | Loss: 0.0241 | Val Loss: 0.0214\n",
            "Epoch 068 | Loss: 0.0239 | Val Loss: 0.0214\n",
            "Epoch 069 | Loss: 0.0241 | Val Loss: 0.0216\n",
            "Epoch 070 | Loss: 0.0237 | Val Loss: 0.0216\n",
            "Epoch 071 | Loss: 0.0236 | Val Loss: 0.0211\n",
            "Epoch 072 | Loss: 0.0237 | Val Loss: 0.0221\n",
            "Epoch 073 | Loss: 0.0236 | Val Loss: 0.0210\n",
            "Epoch 074 | Loss: 0.0235 | Val Loss: 0.0211\n",
            "Epoch 075 | Loss: 0.0234 | Val Loss: 0.0211\n",
            "Epoch 076 | Loss: 0.0235 | Val Loss: 0.0210\n",
            "Epoch 077 | Loss: 0.0234 | Val Loss: 0.0207\n",
            "Epoch 078 | Loss: 0.0232 | Val Loss: 0.0208\n",
            "Epoch 079 | Loss: 0.0231 | Val Loss: 0.0209\n",
            "Epoch 080 | Loss: 0.0230 | Val Loss: 0.0207\n",
            "Epoch 081 | Loss: 0.0228 | Val Loss: 0.0207\n",
            "Epoch 082 | Loss: 0.0229 | Val Loss: 0.0205\n",
            "Epoch 083 | Loss: 0.0230 | Val Loss: 0.0208\n",
            "Epoch 084 | Loss: 0.0228 | Val Loss: 0.0202\n",
            "Epoch 085 | Loss: 0.0226 | Val Loss: 0.0202\n",
            "Epoch 086 | Loss: 0.0226 | Val Loss: 0.0199\n",
            "Epoch 087 | Loss: 0.0225 | Val Loss: 0.0201\n",
            "Epoch 088 | Loss: 0.0226 | Val Loss: 0.0209\n",
            "Epoch 089 | Loss: 0.0224 | Val Loss: 0.0203\n",
            "Epoch 090 | Loss: 0.0225 | Val Loss: 0.0208\n",
            "Epoch 091 | Loss: 0.0222 | Val Loss: 0.0199\n",
            "Epoch 092 | Loss: 0.0223 | Val Loss: 0.0201\n",
            "Epoch 093 | Loss: 0.0221 | Val Loss: 0.0196\n",
            "Epoch 094 | Loss: 0.0222 | Val Loss: 0.0203\n",
            "Epoch 095 | Loss: 0.0222 | Val Loss: 0.0200\n",
            "Epoch 096 | Loss: 0.0221 | Val Loss: 0.0201\n",
            "Epoch 097 | Loss: 0.0220 | Val Loss: 0.0196\n",
            "Epoch 098 | Loss: 0.0219 | Val Loss: 0.0197\n",
            "Epoch 099 | Loss: 0.0221 | Val Loss: 0.0199\n",
            "Epoch 100 | Loss: 0.0218 | Val Loss: 0.0197\n",
            "\n",
            "📊 Resultados Finais:\n",
            "════════════════════════════════════════\n",
            "XGBoost:\n",
            "• RMSE (€): 40.65\n",
            "• MAE (€): 21.64\n",
            "• R² Score: 0.9740\n",
            "\n",
            "Rede Neural:\n",
            "• RMSE (€): 47.54\n",
            "• MAE (€): 25.79\n",
            "• R² Score: 0.9645\n",
            "\n",
            "✅ Processo concluído!\n"
          ]
        }
      ],
      "source": [
        "# 1: Configuração inicial\n",
        "!nvidia-smi\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --upgrade xgboost\n",
        "\n",
        "\n",
        "# 2: Importações e verificação de GPU\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import xgboost as xgb\n",
        "from xgboost import DMatrix\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Verificar GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Dispositivo utilizado: {device}')\n",
        "if str(device) == 'cpu':\n",
        "    print(\"\\n⚠️ ATENÇÃO: GPU não detectada!\")\n",
        "    print(\"1. Vá em Runtime > Change runtime type\")\n",
        "    print(\"2. Selecione 'GPU' em Hardware accelerator\")\n",
        "    print(\"3. Reinicie o runtime (Runtime > Restart runtime)\")\n",
        "\n",
        "\n",
        "# 3: Configurações e pré-processamento\n",
        "TAXA_CONVERSAO = 90\n",
        "\n",
        "def preprocessar_dados(df):\n",
        "    try:\n",
        "        # Conversão de moeda\n",
        "        df['price'] = df['price'] / TAXA_CONVERSAO\n",
        "\n",
        "        # Remoção de colunas\n",
        "        df = df.drop(columns=['Unnamed: 0', 'flight'], errors='ignore')\n",
        "\n",
        "        # Mapeamento de categorias\n",
        "        time_map = {'Early_Morning':0, 'Morning':1, 'Afternoon':2, 'Evening':3, 'Night':4}\n",
        "        stops_map = {'zero':0, 'one':1, 'two_or_more':2}\n",
        "\n",
        "        df['departure_time'] = df['departure_time'].map(time_map).fillna(4)\n",
        "        df['arrival_time'] = df['arrival_time'].map(time_map).fillna(4)\n",
        "        df['stops'] = df['stops'].map(stops_map).fillna(2)\n",
        "\n",
        "        # Codificação de labels\n",
        "        cat_cols = ['airline', 'source_city', 'destination_city', 'class']\n",
        "        for col in cat_cols:\n",
        "            df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
        "\n",
        "        # Limpeza final\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "        df.dropna(inplace=True)\n",
        "        df['price'] = np.log1p(df['price'])\n",
        "\n",
        "        return df.drop('price', axis=1), df['price']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro no pré-processamento: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# 4: Dataset e modelo neural\n",
        "class FlightDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class FlightPriceNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.01),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_normal_(layer.weight, nonlinearity='leaky_relu')\n",
        "                nn.init.constant_(layer.bias, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# 5: Funções de treino\n",
        "def treinar_xgboost(X_train, y_train, X_val, y_val):\n",
        "    dtrain = DMatrix(X_train, y_train)\n",
        "    dval = DMatrix(X_val, y_val)\n",
        "\n",
        "    params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'tree_method': 'hist',\n",
        "        'device': 'cuda',\n",
        "        'learning_rate': 0.03,\n",
        "        'max_depth': 10,\n",
        "        'min_child_weight': 2,\n",
        "        'subsample': 0.9,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'gamma': 0.1,\n",
        "        'lambda': 1.5,\n",
        "        'alpha': 0.2,\n",
        "        'grow_policy': 'depthwise'\n",
        "    }\n",
        "\n",
        "    model = xgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=2000,\n",
        "        evals=[(dval, 'val')],\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=100\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def treinar_rede_neural(X_train, y_train, X_val, y_val, input_size):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        FlightDataset(X_train, y_train),\n",
        "        batch_size=2048,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        FlightDataset(X_val, y_val),\n",
        "        batch_size=2048,\n",
        "        pin_memory=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    model = FlightPriceNet(input_size).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.1)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "    criterion = nn.HuberLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience, no_improve = 10, 0\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validação\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                val_loss += criterion(model(X_batch), y_batch).item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        print(f'Epoch {epoch+1:03d} | Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f'Early stopping @ {epoch+1}')\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pth', map_location=device, weights_only=True))\n",
        "    return model\n",
        "\n",
        "\n",
        "# 6:Pipeline principal\n",
        "\n",
        "def main():\n",
        "    # Carregar dados\n",
        "    try:\n",
        "        df = pd.read_csv('/content/Clean_Dataset.csv')\n",
        "        print(\"Dados carregados com sucesso!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar dados: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Pré-processamento\n",
        "    X, y = preprocessar_dados(df)\n",
        "\n",
        "    # Split dos dados\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Normalização\n",
        "    scaler = StandardScaler()\n",
        "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
        "    X_val = pd.DataFrame(scaler.transform(X_val), columns=X.columns)\n",
        "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
        "\n",
        "    # Treino dos modelos\n",
        "    print(\"\\n🚀 Iniciando treino do XGBoost...\")\n",
        "    xgb_model = treinar_xgboost(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    print(\"\\n🧠 Iniciando treino da Rede Neural...\")\n",
        "    nn_model = treinar_rede_neural(X_train, y_train, X_val, y_val, X_train.shape[1])\n",
        "\n",
        "    # Avaliação\n",
        "    def avaliar(model, neural=False):\n",
        "        if neural:\n",
        "            model.eval()\n",
        "            test_set = FlightDataset(X_test, y_test)\n",
        "            loader = DataLoader(test_set, batch_size=1024)\n",
        "            preds = []\n",
        "            with torch.no_grad():\n",
        "                for X_batch, _ in loader:\n",
        "                    preds.append(model(X_batch.to(device)).cpu().numpy())\n",
        "            y_pred = np.expm1(np.concatenate(preds))\n",
        "        else:\n",
        "            y_pred = np.expm1(xgb_model.predict(DMatrix(X_test)))\n",
        "\n",
        "        y_true = np.expm1(y_test)\n",
        "        return {\n",
        "            'RMSE (€)': f\"{np.sqrt(mean_squared_error(y_true, y_pred)):.2f}\",\n",
        "            'MAE (€)': f\"{mean_absolute_error(y_true, y_pred):.2f}\",\n",
        "            'R² Score': f\"{r2_score(y_true, y_pred):.4f}\"\n",
        "        }\n",
        "\n",
        "    print(\"\\n📊 Resultados Finais:\")\n",
        "    print(\"═\"*40)\n",
        "    print(\"XGBoost:\")\n",
        "    for k, v in avaliar(xgb_model).items():\n",
        "        print(f\"• {k}: {v}\")\n",
        "\n",
        "    print(\"\\nRede Neural:\")\n",
        "    for k, v in avaliar(nn_model, neural=True).items():\n",
        "        print(f\"• {k}: {v}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    print(\"\\n✅ Processo concluído!\")"
      ]
    }
  ]
}